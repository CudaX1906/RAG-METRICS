{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\"What did the president say about Justice Breyer?\", \n",
    "             \"What did the president say about Intel's CEO?\",\n",
    "             \"What did the president say about gun violence?\",\n",
    "            ]\n",
    "ground_truths = [[\"The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\"],\n",
    "                [\"The president said that Pat Gelsinger is ready to increase Intel's investment to $100 billion.\"],\n",
    "                [\"The president asked Congress to pass proven measures to reduce gun violence.\"]]\n",
    "answers = [\"alkmdsa;lk\", \"jhgfdsgdfgh\", \"zxcvbnm,./\"]\n",
    "contexts = [[\"aklsjdal\"],[\"klasmlda\"],[\"klsmdla\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"question\":  [\"What is the capital of France?\"],\n",
    "    \"answer\": [\"Paris\"],\n",
    "    # \"contexts\": [[\"Paris is the capital of France.\"]],\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The metric [faithfulness] that that is used requires the following additional columns ['contexts'] to be present in the dataset. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     faithfulness,\n\u001b[0;32m      4\u001b[0m     answer_relevancy,\n\u001b[0;32m      5\u001b[0m     context_recall,\n\u001b[0;32m      6\u001b[0m     context_precision,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# context_precision,\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# context_recall,\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# answer_relevancy,\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\polasani rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ragas\\evaluation.py:143\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, is_async, max_workers, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[0;32m    142\u001b[0m dataset \u001b[38;5;241m=\u001b[39m handle_deprecated_ground_truths(dataset)\n\u001b[1;32m--> 143\u001b[0m \u001b[43mvalidate_evaluation_modes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m validate_column_dtypes(dataset)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\polasani rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ragas\\validation.py:94\u001b[0m, in \u001b[0;36mvalidate_evaluation_modes\u001b[1;34m(ds, metrics)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(m, ContextPrecision)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_columns\n\u001b[0;32m     91\u001b[0m ):\n\u001b[0;32m     92\u001b[0m     extra_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooks like you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre trying to use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_precision\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m without ground_truth. Please use consider using  `context_utilization\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe metric [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] that that is used requires the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(required_columns\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mavailable_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be present in the dataset. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: The metric [faithfulness] that that is used requires the following additional columns ['contexts'] to be present in the dataset. "
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        # context_precision,\n",
    "        # context_recall,\n",
    "        faithfulness,\n",
    "        # answer_relevancy,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"faithfulness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\polasani rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:50: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# If running, create a new task and run it on the existing event loop\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main())\n\u001b[1;32m---> 12\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# If no event loop is running, use asyncio.run()\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mrun(main())\n",
      "File \u001b[1;32mc:\\Users\\polasani rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:629\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \n\u001b[0;32m    620\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 629\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[0;32m    632\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\polasani rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:588\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 588\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    590\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    591\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from ragas.metrics._faithfulness import Faithfulness\n",
    "\n",
    "async def main():\n",
    "    faithfulness = Faithfulness()\n",
    "    results = await faithfulness.score(dataset)\n",
    "\n",
    "# Check if an event loop is already running\n",
    "if asyncio.get_event_loop().is_running():\n",
    "    # If running, create a new task and run it on the existing event loop\n",
    "    task = asyncio.ensure_future(main())\n",
    "    asyncio.get_event_loop().run_until_complete(task)\n",
    "else:\n",
    "    # If no event loop is running, use asyncio.run()\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\polasani rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.52k/1.52k [00:00<?, ?B/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 185kB/s]\n",
      "README.md: 100%|██████████| 90.1k/90.1k [00:00<00:00, 3.22MB/s]\n",
      "config.json: 100%|██████████| 719/719 [00:00<?, ?B/s] \n",
      "config_sentence_transformers.json: 100%|██████████| 124/124 [00:00<?, ?B/s] \n",
      "model.safetensors: 100%|██████████| 438M/438M [05:51<00:00, 1.25MB/s] \n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [05:51<00:00, 1.25MB/s] \n",
      "sentence_bert_config.json: 100%|██████████| 52.0/52.0 [00:00<?, ?B/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 1.21MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 998kB/s] \n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 352kB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en')\n",
    "question_vec = embeddings.embed_query(\"question\")\n",
    "answer_vec = embeddings.embed_query(\"answer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 438M/438M [06:26<00:00, 1.13MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 572kB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 1.56MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 136kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# Model name\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save the model and tokenizer using pickle\n",
    "with open(\"model_tokenizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump((model, tokenizer), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pickled_model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_tokenizer.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "pickled_model = pickle.load(open('model_tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BertModel(\n",
       "   (embeddings): BertEmbeddings(\n",
       "     (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "     (position_embeddings): Embedding(512, 768)\n",
       "     (token_type_embeddings): Embedding(2, 768)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (encoder): BertEncoder(\n",
       "     (layer): ModuleList(\n",
       "       (0-11): 12 x BertLayer(\n",
       "         (attention): BertAttention(\n",
       "           (self): BertSelfAttention(\n",
       "             (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (output): BertSelfOutput(\n",
       "             (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (intermediate): BertIntermediate(\n",
       "           (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (intermediate_act_fn): GELUActivation()\n",
       "         )\n",
       "         (output): BertOutput(\n",
       "           (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (pooler): BertPooler(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (activation): Tanh()\n",
       "   )\n",
       " ),\n",
       " BertTokenizerFast(name_or_path='BAAI/bge-base-en', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " })"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickled_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics._answer_relevance import AnswerRelevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms.prompt import Prompt\n",
    "\n",
    "QUESTION_GEN = Prompt(\n",
    "    name=\"question_generation\",\n",
    "    instruction=\"\"\"Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don't know\" or \"I'm not sure\" are noncommittal answers.\"\"\",\n",
    "    examples=[\n",
    "        {\n",
    "            \"answer\": \"\"\"Albert Einstein was born in Germany.\"\"\",\n",
    "            \"context\": \"\"\"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time\"\"\",\n",
    "            \"output\": {\n",
    "                \"question\": \"Where was Albert Einstein born?\",\n",
    "                \"noncommittal\": 0,\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"answer\": \"\"\"It can change its skin color based on the temperature of its environment.\"\"\",\n",
    "            \"context\": \"\"\"A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.\"\"\",\n",
    "            \"output\": {\n",
    "                \"question\": \"What unique ability does the newly discovered species of frog have?\",\n",
    "                \"noncommittal\": 0,\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"answer\": \"\"\"Everest\"\"\",\n",
    "            \"context\": \"\"\"The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas.\"\"\",\n",
    "            \"output\": {\n",
    "                \"question\": \"What is the tallest mountain on Earth?\",\n",
    "                \"noncommittal\": 0,\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"answer\": \"\"\"I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\"\",\n",
    "            \"context\": \"\"\"In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology.\"\"\",\n",
    "            \"output\": {\n",
    "                \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\n",
    "                \"noncommittal\": 1,\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    input_keys=[\"answer\", \"context\"],\n",
    "    output_key=\"output\",\n",
    "    output_type=\"json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ragas.llms.prompt.Prompt"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(QUESTION_GEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = QUESTION_GEN.format(answer=\"newdelhi\", context=\"Paris is the capital of France.new delhi is also a capital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = QUESTION_GEN.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms.base import LangchainLLMWrapper\n",
    "class  LLMSRagAsm(LangchainLLMWrapper):\n",
    "    def __init__(self,prompt):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def gen(self):\n",
    "        ans = LangchainLLMWrapper(langchain_llm=OpenAI())\n",
    "\n",
    "        return ans.generate_text(prompt=self.prompt,n=3)\n",
    "\n",
    "obj = LLMSRagAsm(prompt=prompt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"question\": \"What is the capital of France?\", \"noncommittal\": 0}, {\"question\": \"What is the capital of India?\", \"noncommittal\": 1}, {\"question\": \"Can you name a city that is a capital?\", \"noncommittal\": 1}]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.gen().generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
